To implement the LRU cache in Python, we can use a combination of a dictionary and a doubly linked list. The dictionary will allow for constant time access to cache items, while the doubly linked list will help maintain the order of recently used items.

Here's the implementation:

```python
class Node:
    def __init__(self, key, value):
        self.key = key
        self.value = value
        self.prev = None
        self.next = None


class LRUCache:
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache = {}
        self.head = Node(0, 0)  # dummy head node
        self.tail = Node(0, 0)  # dummy tail node
        self.head.next = self.tail
        self.tail.prev = self.head

    def _add_node(self, node):
        # Add a node to the front of the doubly linked list
        node.prev = self.head
        node.next = self.head.next
        self.head.next.prev = node
        self.head.next = node

    def _remove_node(self, node):
        # Remove a node from the doubly linked list
        node.prev.next = node.next
        node.next.prev = node.prev

    def _move_to_front(self, node):
        # Move a node to the front of the doubly linked list
        self._remove_node(node)
        self._add_node(node)

    def _pop_tail(self):
        # Remove and return the tail node from the doubly linked list
        tail = self.tail.prev
        self._remove_node(tail)
        return tail

    def get(self, key: int) -> int:
        if key in self.cache:
            node = self.cache[key]
            self._move_to_front(node)
            return node.value
        else:
            return -1

    def put(self, key: int, value: int) -> None:
        if key in self.cache:
            node = self.cache[key]
            node.value = value
            self._move_to_front(node)
        else:
            if len(self.cache) >= self.capacity:
                # Remove the least recently used (LRU) item
                tail = self._pop_tail()
                del self.cache[tail.key]
            new_node = Node(key, value)
            self.cache[key] = new_node
            self._add_node(new_node)
```

Here's an example usage of the `LRUCache` class:

```python
lru_cache = LRUCache(2)
lru_cache.put(1, 1)
lru_cache.put(2, 2)
print(lru_cache.get(1))  # Output: 1
lru_cache.put(3, 3)
print(lru_cache.get(2))  # Output: -1
lru_cache.put(4, 4)
print(lru_cache.get(1))  # Output: -1
print(lru_cache.get(3))  # Output: 3
print(lru_cache.get(4))  # Output: 4
```

This implementation should satisfy the requirements of the LRU cache, with the `get` and `put` operations running in constant time complexity, O(1), on average.