To solve the task, we can utilize the following approach:

1. Create an array `ans` of length `n + 1` and initialize it with all zeros.
2. Iterate from 1 to `n` (inclusive).
3. For each iteration, set `ans[i]` equal to `ans[i >> 1] + (i & 1)`.
   - `ans[i >> 1]` represents the count of 1's in the binary representation of `i` without its least significant bit.
   - `(i & 1)` gives the least significant bit of `i` (either 0 or 1), which is added to the count.
4. Return the resulting array `ans`.

Here's the implementation of the `countBits` function in Python:

```python
from typing import List

class Solution:
    def countBits(self, n: int) -> List[int]:
        ans = [0] * (n + 1)
        for i in range(1, n + 1):
            ans[i] = ans[i >> 1] + (i & 1)
        return ans
```

You can create an instance of the `Solution` class and call the `countBits` function with the desired `n` value to obtain the array with the counts of 1's in the binary representation of each number from 0 to `n`. For example:

```python
solution = Solution()
print(solution.countBits(2))  # Output: [0, 1, 1]
print(solution.countBits(5))  # Output: [0, 1, 1, 2, 1, 2]
```

This solution has a linear time complexity of O(n) and solves the task in a single pass, fulfilling the requirements.